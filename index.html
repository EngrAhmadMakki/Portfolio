<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <!-- SEO & Social Meta Tags -->
    <title>Ahmad Makki | AI/ML Engineer</title>
    <meta name="description"
          content="Portfolio of Ahmad Makki, an AI/ML Engineer specializing in Computer Vision, NLP, LLM Apps, and MLOps." />
    <meta property="og:title" content="Ahmad Makki | AI/ML Engineer Portfolio" />
    <meta property="og:description"
          content="Explore my projects in real-time computer vision, generative AI, and MLOps using Python, Docker, and Kubernetes." />
    <meta property="og:image" content="Ahmad_Makki_Pic.jpeg" />
    <meta property="og:type" content="website" />

    <!-- Favicon (Base64 encoded SVG) -->
    <link rel="icon"
          href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 128 128'><rect width='128' height='128' fill='%2308243a'/><text x='50%' y='50%' dominant-baseline='middle' text-anchor='middle' fill='%23ffffff' font-size='56' font-family='sans-serif'>A</text></svg>">

    <!-- TailwindCSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- Tailwind Config (if you had one, keep it) -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        brand: {
                            50: '#ecfeff',
                            100: '#cffafe',
                            200: '#a5f3fc',
                            300: '#67e8f9',
                            400: '#22d3ee',
                            500: '#06b6d4',
                            600: '#0891b2',
                            700: '#0e7490',
                            800: '#155e75',
                            900: '#164e63',
                        },
                    },
                },
            },
        };
    </script>

    <!-- TensorFlow & Models -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@latest/dist/coco-ssd.min.js"></script>
    <!-- NEW: Pose Estimation & Segmentation Models -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix"></script>

    <!-- Base styles for dark background -->
    <style>
        html, body {
            scroll-behavior: smooth;
        }
    </style>
</head>
<body class="bg-slate-950 text-slate-100 antialiased min-h-screen">

<!-- Top Navigation -->
<header class="sticky top-0 z-40 border-b border-slate-800/60 bg-slate-950/85 backdrop-blur">
    <div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 flex items-center justify-between h-16">
        <a href="#top" class="flex items-center gap-2 group">
            <div class="w-9 h-9 rounded-xl bg-cyan-500/10 border border-cyan-500/30 flex items-center justify-center">
                <span class="text-cyan-400 font-black text-lg">A</span>
            </div>
            <div class="flex flex-col leading-tight">
                <span class="text-sm font-semibold text-slate-100 group-hover:text-cyan-400 transition-colors">Ahmad
                    Makki</span>
                <span class="text-[11px] text-slate-400">AI/ML Engineer</span>
            </div>
        </a>

        <nav class="hidden md:flex items-center gap-6 text-sm">
            <a href="#about" class="text-slate-300 hover:text-cyan-400 transition-colors">About</a>
            <a href="#skills" class="text-slate-300 hover:text-cyan-400 transition-colors">Skills</a>
            <a href="#experience" class="text-slate-300 hover:text-cyan-400 transition-colors">Experience</a>
            <a href="#projects" class="text-slate-300 hover:text-cyan-400 transition-colors">Projects</a>
            <a href="#ai-playground" class="text-slate-300 hover:text-cyan-400 transition-colors">AI Playground</a>
            <a href="#resume-analyzer" class="text-slate-300 hover:text-cyan-400 transition-colors">JD Match</a>
            <a href="#contact" class="text-slate-300 hover:text-cyan-400 transition-colors">Contact</a>
        </nav>
    </div>
</header>

<main id="top" class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 pb-24">

    <!-- Hero -->
    <section class="pt-12 md:pt-16 lg:pt-20 flex flex-col md:flex-row items-center gap-10">
        <div class="flex-1">
            <p class="text-xs font-semibold tracking-[0.25em] text-cyan-400 uppercase mb-3">
                AI / ML • Computer Vision • LLM Apps
            </p>
            <h1 class="text-3xl sm:text-4xl lg:text-5xl font-bold tracking-tight text-slate-50">
                Building real-time <span class="text-cyan-400">Computer Vision</span> &
                <span class="text-cyan-400">LLM</span> systems that ship.
            </h1>
            <p class="mt-4 text-sm sm:text-base text-slate-300 max-w-xl">
                I’m an AI/ML Engineer focused on retail shelf analytics, object detection, and practical LLM
                integrations. From YOLO-based vision systems to workflow automation with LangChain and n8n, I turn
                messy real-world problems into working products.
            </p>
            <div class="mt-6 flex flex-wrap items-center gap-3">
                <a href="#projects"
                   class="inline-flex items-center gap-2 bg-cyan-500 text-slate-900 font-semibold px-5 py-2.5 rounded-lg text-sm shadow-lg shadow-cyan-500/20 hover:bg-cyan-400 transition-all duration-300 transform hover:scale-105">
                    View Projects
                </a>
                <a href="#contact"
                   class="inline-flex items-center gap-2 border border-slate-700 text-slate-200 font-medium px-4 py-2.5 rounded-lg text-sm hover:border-cyan-500 hover:text-cyan-400 transition-all duration-300">
                    Contact
                </a>
                <span class="text-[11px] text-slate-500">
                    Available for AI/ML roles & consulting.
                </span>
            </div>
        </div>

        <div class="flex-1 flex justify-center md:justify-end">
            <div
                class="relative w-56 h-56 sm:w-64 sm:h-64 rounded-3xl bg-gradient-to-br from-cyan-500/20 via-slate-900 to-slate-900 border border-cyan-500/30 shadow-[0_0_60px_rgba(8,145,178,0.35)] overflow-hidden flex items-center justify-center">
                <img src="Ahmad_Makki_Pic.jpeg" alt="Ahmad Makki"
                     class="w-40 h-40 rounded-2xl object-cover border border-slate-700/80">
                <div class="absolute bottom-3 left-3 right-3 flex flex-col gap-1">
                    <div
                        class="flex items-center justify-between text-[11px] px-3 py-1.5 rounded-xl bg-slate-900/85 border border-slate-700/80">
                        <span class="text-slate-200 font-semibold">Real-time CV</span>
                        <span class="text-cyan-400 font-mono">YOLO • SIFT • ViT</span>
                    </div>
                    <div
                        class="flex items-center justify-between text-[11px] px-3 py-1.5 rounded-xl bg-slate-900/85 border border-slate-700/80">
                        <span class="text-slate-200 font-semibold">LLM Apps</span>
                        <span class="text-cyan-400 font-mono">Gemini • OpenAI</span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- About -->
    <section id="about" class="py-16 md:py-24 border-b border-slate-800/50">
        <h2 class="text-2xl sm:text-3xl font-bold tracking-tight text-slate-100 mb-6">About</h2>
        <p class="text-sm sm:text-base text-slate-300 max-w-3xl">
            I design and deploy end-to-end AI systems: dataset strategy, annotation pipelines, model training, and
            production APIs. Recently, I’ve been leading shelf analytics and parking occupancy projects using YOLO,
            SIFT, and transformer-based models, plus building workflow automation on top of LLMs (Gemini, GPT) for
            agencies and retail clients.
        </p>
    </section>

    <!-- Skills -->
    <section id="skills" class="py-16 md:py-24 border-b border-slate-800/50">
        <h2 class="text-2xl sm:text-3xl font-bold tracking-tight text-slate-100 mb-6">Skills</h2>
        <div class="grid gap-6 md:grid-cols-3 text-sm">
            <div class="bg-slate-900/60 border border-slate-800 rounded-2xl p-5">
                <h3 class="font-semibold text-cyan-400 mb-2 text-sm uppercase tracking-wide">Core</h3>
                <ul class="space-y-1.5 text-slate-300">
                    <li>Python, FastAPI, Node.js</li>
                    <li>Computer Vision (YOLO, SIFT, ViT)</li>
                    <li>LLM Apps (Gemini, OpenAI)</li>
                    <li>LangChain, RAG, Embeddings</li>
                </ul>
            </div>
            <div class="bg-slate-900/60 border border-slate-800 rounded-2xl p-5">
                <h3 class="font-semibold text-cyan-400 mb-2 text-sm uppercase tracking-wide">MLOps & Infra</h3>
                <ul class="space-y-1.5 text-slate-300">
                    <li>Docker, Uvicorn, REST APIs</li>
                    <li>GPU training (Colab, on-prem)</li>
                    <li>Ultralytics Hub, Roboflow</li>
                    <li>n8n, Notion, Zoho integrations</li>
                </ul>
            </div>
            <div class="bg-slate-900/60 border border-slate-800 rounded-2xl p-5">
                <h3 class="font-semibold text-cyan-400 mb-2 text-sm uppercase tracking-wide">Domain</h3>
                <ul class="space-y-1.5 text-slate-300">
                    <li>Retail shelf analytics & planograms</li>
                    <li>Parking / people analytics</li>
                    <li>Agency workflow automation</li>
                    <li>Real estate & construction ops</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Experience -->
    <section id="experience" class="py-16 md:py-24 border-b border-slate-800/50">
        <h2 class="text-2xl sm:text-3xl font-bold tracking-tight text-slate-100 mb-6">Experience</h2>
        <div class="space-y-6 text-sm">
            <div class="bg-slate-900/60 border border-slate-800 rounded-2xl p-5">
                <div class="flex flex-wrap justify-between gap-2 mb-1">
                    <h3 class="font-semibold text-slate-100">AI/ML Engineer</h3>
                    <span class="text-[11px] text-slate-400">2024 — Present</span>
                </div>
                <p class="text-xs text-cyan-400 mb-2">Retail analytics, CV models, LLM automation</p>
                <ul class="list-disc list-inside space-y-1.5 text-slate-300">
                    <li>Led shelf analytics projects (honey, olive oil, multibrand) using YOLO, SIFT & custom zone
                        detection.
                    </li>
                    <li>Designed dataset & annotation strategy (1,500–2,000 images per sub-category, QC loops).</li>
                    <li>Built FastAPI microservices for real-time inference with Dockerized deployment.</li>
                    <li>Prototyped LLM-based workflows for agencies: WhatsApp → n8n → Notion → Zoho Calendar.</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Projects -->
    <section id="projects" class="py-16 md:py-24 border-b border-slate-800/50">
        <h2 class="text-2xl sm:text-3xl font-bold tracking-tight text-slate-100 mb-6">Highlighted Projects</h2>
        <div class="grid gap-6 md:grid-cols-2 text-sm">
            <article class="bg-slate-900/60 border border-slate-800 rounded-2xl p-5">
                <h3 class="font-semibold text-cyan-400 mb-1">Superstore Shelf Analytics</h3>
                <p class="text-xs text-slate-400 mb-2">
                    YOLO-based multi-model pipeline for SKUs, shelf bands, and promo tags.
                </p>
                <ul class="list-disc list-inside space-y-1 text-slate-300">
                    <li>Brand-level facings & zone counts (Top/Middle/Bottom).</li>
                    <li>Planogram compliance and promo presence analytics.</li>
                    <li>Roboflow + on-prem GPU training loop.</li>
                </ul>
            </article>
            <article class="bg-slate-900/60 border border-slate-800 rounded-2xl p-5">
                <h3 class="font-semibold text-cyan-400 mb-1">AI Agency Workflow Automation</h3>
                <p class="text-xs text-slate-400 mb-2">
                    WhatsApp-first automation with LLM parsing and project orchestration.
                </p>
                <ul class="list-disc list-inside space-y-1 text-slate-300">
                    <li>n8n flows for intake, task creation, reminders.</li>
                    <li>LLM (Gemini) for JSON extraction from voice/text briefs.</li>
                    <li>Notion + Zoho Calendar as single source of truth.</li>
                </ul>
            </article>
        </div>
    </section>

    <!-- AI Playground: Object Detection + Pose + Segmentation -->
    <section id="ai-playground" class="py-16 md:py-24 border-b border-slate-800/50">
        <div class="text-center">
            <h2 class="text-3xl font-bold tracking-tight text-slate-100 sm:text-4xl">AI Playground</h2>
            <p class="mt-4 text-lg text-cyan-400">Live In-Browser AI Demos</p>
            <p class="mt-2 text-sm text-slate-400 max-w-xl mx-auto">
                All demos run directly in your browser using TensorFlow.js. No server calls, no data leaves your device.
            </p>
        </div>

        <div class="mt-12 max-w-6xl mx-auto grid gap-8 md:grid-cols-2 xl:grid-cols-3">
            <!-- Demo 1: Object Detection (COCO-SSD) -->
            <div class="bg-slate-800/50 p-6 rounded-2xl shadow-xl border border-slate-700/50 flex flex-col">
                <h3 class="text-lg font-semibold text-cyan-400 mb-2">Object Detection</h3>
                <p class="text-xs text-slate-400 mb-4">
                    COCO-SSD model detects everyday objects in real-time from your webcam feed.
                </p>
                <div class="text-center mb-4">
                    <button id="webcam-button"
                            class="inline-flex items-center gap-2 bg-cyan-500 text-slate-900 font-bold px-4 py-2 rounded-lg text-xs shadow-lg hover:bg-cyan-400 transition-all duration-300 transform hover:scale-105">
                        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"
                             viewBox="0 0 24 24" fill="none" stroke="currentColor"
                             stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                             class="lucide lucide-camera">
                            <path
                                d="M14.5 4h-5L7 7H4a2 2 0 0 0-2 2v9a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2h-3l-2.5-3z"/>
                            <circle cx="12" cy="13" r="3"/>
                        </svg>
                        Start / Stop
                    </button>
                    <p id="model-status" class="text-[11px] text-slate-500 mt-2 h-4"></p>
                </div>
                <div
                    class="relative w-full aspect-video bg-slate-900 rounded-lg overflow-hidden border border-slate-700">
                    <video id="webcam-feed" autoplay muted playsinline
                           class="w-full h-full object-cover"></video>
                    <canvas id="detection-canvas"
                            class="absolute top-0 left-0 w-full h-full"></canvas>
                </div>
            </div>

            <!-- Demo 2: Pose Estimation (MoveNet) -->
            <div class="bg-slate-800/50 p-6 rounded-2xl shadow-xl border border-slate-700/50 flex flex-col">
                <h3 class="text-lg font-semibold text-cyan-400 mb-2">Pose Estimation</h3>
                <p class="text-xs text-slate-400 mb-4">
                    MoveNet estimates your body keypoints and draws a live skeleton overlay.
                </p>
                <div class="text-center mb-4">
                    <button id="pose-button"
                            class="inline-flex items-center gap-2 bg-cyan-500 text-slate-900 font-bold px-4 py-2 rounded-lg text-xs shadow-lg hover:bg-cyan-400 transition-all duration-300 transform hover:scale-105">
                        Start / Stop
                    </button>
                    <p id="pose-status" class="text-[11px] text-slate-500 mt-2 h-4"></p>
                </div>
                <div
                    class="relative w-full aspect-video bg-slate-900 rounded-lg overflow-hidden border border-slate-700">
                    <video id="pose-video" autoplay muted playsinline
                           class="w-full h-full object-cover"></video>
                    <canvas id="pose-canvas"
                            class="absolute top-0 left-0 w-full h-full"></canvas>
                </div>
            </div>

            <!-- Demo 3: Person Segmentation (BodyPix) -->
            <div class="bg-slate-800/50 p-6 rounded-2xl shadow-xl border border-slate-700/50 flex flex-col">
                <h3 class="text-lg font-semibold text-cyan-400 mb-2">Person Segmentation</h3>
                <p class="text-xs text-slate-400 mb-4">
                    BodyPix segments you from the background and highlights the person region.
                </p>
                <div class="text-center mb-4">
                    <button id="seg-button"
                            class="inline-flex items-center gap-2 bg-cyan-500 text-slate-900 font-bold px-4 py-2 rounded-lg text-xs shadow-lg hover:bg-cyan-400 transition-all duration-300 transform hover:scale-105">
                        Start / Stop
                    </button>
                    <p id="seg-status" class="text-[11px] text-slate-500 mt-2 h-4"></p>
                </div>
                <div
                    class="relative w-full aspect-video bg-slate-900 rounded-lg overflow-hidden border border-slate-700">
                    <video id="seg-video" autoplay muted playsinline
                           class="w-full h-full object-cover"></video>
                    <canvas id="seg-canvas"
                            class="absolute top-0 left-0 w-full h-full"></canvas>
                </div>
            </div>
        </div>
    </section>

    <!-- NEW: Resume & JD Analyzer (Gemini) -->
    <section id="resume-analyzer" class="py-16 md:py-24 border-b border-slate-800/50">
        <div class="text-center">
            <h2 class="text-3xl font-bold tracking-tight text-slate-100 sm:text-4xl">
                AI Resume & JD Match Analyzer
            </h2>
            <p class="mt-4 text-lg text-cyan-400">
                Paste a Job Description and your Resume – let my AI estimate the fit.
            </p>
            <p class="mt-2 text-sm text-slate-400 max-w-2xl mx-auto">
                This uses the same Gemini model I integrate into client projects. All processing happens via API;
                nothing is stored on my server.
            </p>
        </div>

        <div class="mt-10 max-w-5xl mx-auto grid gap-8 md:grid-cols-2">
            <!-- JD -->
            <div>
                <label for="jd-input" class="block text-sm font-medium text-slate-200 mb-2">
                    Job Description
                </label>
                <textarea id="jd-input" rows="10"
                          class="w-full px-4 py-3 rounded-lg bg-slate-900 border border-slate-700 text-slate-200 text-sm placeholder-slate-500 focus:outline-none focus:ring-2 focus:ring-cyan-500 focus:border-cyan-500"
                          placeholder="Paste the JD here..."></textarea>
            </div>
            <!-- Resume -->
            <div>
                <label for="resume-input" class="block text-sm font-medium text-slate-200 mb-2">
                    Your Resume (Text)
                </label>
                <textarea id="resume-input" rows="10"
                          class="w-full px-4 py-3 rounded-lg bg-slate-900 border border-slate-700 text-slate-200 text-sm placeholder-slate-500 focus:outline-none focus:ring-2 focus:ring-cyan-500 focus:border-cyan-500"
                          placeholder="Paste your resume content here..."></textarea>
            </div>
        </div>

        <div class="mt-6 max-w-5xl mx-auto flex flex-col md:flex-row items-start gap-4">
            <button id="analyze-resume-button"
                    class="inline-flex items-center gap-2 bg-cyan-500 text-slate-900 font-bold px-6 py-3 rounded-lg text-sm shadow-lg hover:bg-cyan-400 transition-all duration-300 transform hover:scale-105">
                Analyze Match
            </button>
            <p class="text-xs text-slate-500">
                Tip: This is a demo. In real projects, I plug similar flows into ATS or internal HR tools.
            </p>
        </div>

        <div class="mt-8 max-w-5xl mx-auto bg-slate-900/70 border border-slate-700/70 rounded-2xl p-6">
            <h3 class="text-sm font-semibold text-cyan-400 mb-2">AI Analysis</h3>
            <div id="resume-analysis-output"
                 class="text-sm text-slate-200 whitespace-pre-wrap min-h-[4rem]">
                Paste a JD and your resume, then click “Analyze Match”.
            </div>
        </div>
    </section>

    <!-- Education -->
    <section id="education" class="py-16 md:py-24 border-b border-slate-800/50">
        <h2 class="text-2xl sm:text-3xl font-bold tracking-tight text-slate-100 mb-6">Education</h2>
        <div class="space-y-4 text-sm">
            <div class="bg-slate-900/60 border border-slate-800 rounded-2xl p-5">
                <div class="flex flex-wrap justify-between gap-2 mb-1">
                    <h3 class="font-semibold text-slate-100">BS Computer Science (ERP)</h3>
                    <span class="text-[11px] text-slate-400">COMSATS University, Lahore</span>
                </div>
                <p class="text-xs text-slate-400">
                    Strong foundation in software engineering, databases, and enterprise systems.
                </p>
            </div>
        </div>
    </section>

    <!-- Contact -->
    <section id="contact" class="py-16 md:py-24">
        <h2 class="text-2xl sm:text-3xl font-bold tracking-tight text-slate-100 mb-6">Contact</h2>
        <div class="grid gap-6 md:grid-cols-2 text-sm">
            <div class="space-y-2 text-slate-300">
                <p><span class="font-semibold text-slate-100">Name:</span> Ahmad Makki</p>
                <p>
                    <span class="font-semibold text-slate-100">Email:</span>
                    <a href="mailto:kh.talha.humayun@gmail.com"
                       class="text-cyan-400 hover:text-cyan-300">
                        kh.talha.humayun@gmail.com
                    </a>
                </p>
                <p>
                    <span class="font-semibold text-slate-100">LinkedIn:</span>
                    <a href="https://www.linkedin.com/in/iKhawajaTalha" target="_blank"
                       class="text-cyan-400 hover:text-cyan-300">
                        linkedin.com/in/iKhawajaTalha
                    </a>
                </p>
            </div>
            <form class="space-y-3">
                <div>
                    <label class="block text-xs font-medium text-slate-300 mb-1" for="contact-name">Name</label>
                    <input id="contact-name" type="text"
                           class="w-full px-3 py-2 rounded-lg bg-slate-900 border border-slate-800 text-slate-100 text-sm focus:outline-none focus:ring-2 focus:ring-cyan-500 focus:border-cyan-500"
                           placeholder="Your name" />
                </div>
                <div>
                    <label class="block text-xs font-medium text-slate-300 mb-1" for="contact-email">Email</label>
                    <input id="contact-email" type="email"
                           class="w-full px-3 py-2 rounded-lg bg-slate-900 border border-slate-800 text-slate-100 text-sm focus:outline-none focus:ring-2 focus:ring-cyan-500 focus:border-cyan-500"
                           placeholder="you@example.com" />
                </div>
                <div>
                    <label class="block text-xs font-medium text-slate-300 mb-1" for="contact-message">Message</label>
                    <textarea id="contact-message" rows="4"
                              class="w-full px-3 py-2 rounded-lg bg-slate-900 border border-slate-800 text-slate-100 text-sm focus:outline-none focus:ring-2 focus:ring-cyan-500 focus:border-cyan-500"
                              placeholder="Tell me a bit about your project or role..."></textarea>
                </div>
                <button type="button"
                        class="inline-flex items-center justify-center bg-cyan-500 text-slate-900 font-semibold px-4 py-2.5 rounded-lg text-sm shadow-lg hover:bg-cyan-400 transition-all duration-300 transform hover:scale-105">
                    Send (demo)
                </button>
            </form>
        </div>
    </section>
</main>

<!-- Footer -->
<footer class="border-t border-slate-800/70 py-6">
    <div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 flex flex-col sm:flex-row items-center justify-between gap-3 text-xs text-slate-500">
        <p>© <span id="year-span"></span> Ahmad Makki. All rights reserved.</p>
        <p>Built with TailwindCSS, TensorFlow.js & Gemini.</p>
    </div>
</footer>

<!-- Scripts -->
<script>
    // Set current year in footer
    document.getElementById('year-span').textContent = new Date().getFullYear();

    // Gemini API Setup
    const GEMINI_API_KEY = "YOUR_GEMINI_API_KEY_HERE"; // replace with your key
    const GEMINI_TEXT_API_URL =
        `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${GEMINI_API_KEY}`;

    async function fetchWithBackoff(url, options = {}, retries = 2, backoffMs = 800) {
        try {
            const res = await fetch(url, options);
            if (!res.ok && retries > 0 && res.status >= 500) {
                await new Promise(r => setTimeout(r, backoffMs));
                return fetchWithBackoff(url, options, retries - 1, backoffMs * 2);
            }
            return res;
        } catch (err) {
            if (retries > 0) {
                await new Promise(r => setTimeout(r, backoffMs));
                return fetchWithBackoff(url, options, retries - 1, backoffMs * 2);
            }
            throw err;
        }
    }

    document.addEventListener('DOMContentLoaded', () => {
        // --- Object Detection Demo (COCO-SSD) ---
        const video = document.getElementById('webcam-feed');
        const canvas = document.getElementById('detection-canvas');
        const ctx = canvas.getContext('2d');
        const button = document.getElementById('webcam-button');
        const statusEl = document.getElementById('model-status');

        let model = null;
        let stream = null;
        let animationId = null;
        let isRunning = false;

        async function loadObjectDetectionModel() {
            if (model) return model;
            statusEl.textContent = 'Loading COCO-SSD...';
            try {
                model = await cocoSsd.load();
                statusEl.textContent = 'Model loaded.';
            } catch (err) {
                console.error('Error loading coco-ssd model:', err);
                statusEl.textContent = 'Error loading model.';
            }
            return model;
        }

        async function startWebcamDetection() {
            if (!navigator.mediaDevices?.getUserMedia) {
                statusEl.textContent = 'Webcam not supported in this browser.';
                return;
            }
            try {
                stream = await navigator.mediaDevices.getUserMedia({
                    video: { width: { ideal: 640 }, height: { ideal: 480 }, facingMode: 'user' }
                });
                video.srcObject = stream;
                video.onloadedmetadata = () => {
                    video.play();
                    canvas.width = video.videoWidth;
                    canvas.height = video.videoHeight;
                    statusEl.textContent = 'Detecting objects...';
                    detectFrame();
                };
            } catch (err) {
                console.error('Error accessing webcam:', err);
                statusEl.textContent = 'Error accessing webcam.';
            }
        }

        function stopWebcamDetection() {
            if (animationId) cancelAnimationFrame(animationId);
            animationId = null;
            if (stream) {
                stream.getTracks().forEach(t => t.stop());
                stream = null;
            }
            video.srcObject = null;
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            statusEl.textContent = 'Stopped.';
        }

        async function detectFrame() {
            if (!model || !video || video.readyState < 2) {
                animationId = requestAnimationFrame(detectFrame);
                return;
            }
            try {
                const predictions = await model.detect(video);
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

                ctx.lineWidth = 2;
                ctx.font = '12px system-ui';
                predictions.forEach(pred => {
                    const [x, y, w, h] = pred.bbox;
                    ctx.strokeStyle = '#22d3ee';
                    ctx.fillStyle = 'rgba(15,23,42,0.75)';
                    ctx.strokeRect(x, y, w, h);
                    const text = `${pred.class} ${(pred.score * 100).toFixed(1)}%`;
                    const textWidth = ctx.measureText(text).width + 8;
                    const textHeight = 18;
                    ctx.fillRect(x, y - textHeight, textWidth, textHeight);
                    ctx.fillStyle = '#e2e8f0';
                    ctx.fillText(text, x + 4, y - 5);
                });
            } catch (err) {
                console.error('Detection error:', err);
            }
            animationId = requestAnimationFrame(detectFrame);
        }

        if (button) {
            button.addEventListener('click', async () => {
                if (!isRunning) {
                    isRunning = true;
                    await loadObjectDetectionModel();
                    await startWebcamDetection();
                    button.textContent = 'Stop';
                } else {
                    isRunning = false;
                    stopWebcamDetection();
                    button.textContent = 'Start / Stop';
                }
            });
        }

        // --- Pose Estimation Demo (MoveNet) ---
        const poseVideo = document.getElementById('pose-video');
        const poseCanvas = document.getElementById('pose-canvas');
        const poseButton = document.getElementById('pose-button');
        const poseStatus = document.getElementById('pose-status');
        const poseCtx = poseCanvas.getContext('2d');

        let poseModel = null;
        let poseAnimId = null;
        let poseStream = null;

        async function loadPoseModel() {
            if (poseModel) return poseModel;
            poseStatus.textContent = 'Loading pose model...';
            try {
                poseModel = await poseDetection.createDetector(
                    poseDetection.SupportedModels.MoveNet,
                    { modelType: poseDetection.movenet.modelType.SINGLEPOSE_LIGHTNING }
                );
                poseStatus.textContent = 'Model ready.';
            } catch (err) {
                console.error('Error loading pose model:', err);
                poseStatus.textContent = 'Error loading pose model.';
            }
            return poseModel;
        }

        async function startPoseWebcam() {
            if (!navigator.mediaDevices?.getUserMedia) {
                poseStatus.textContent = 'Webcam not supported.';
                return;
            }
            try {
                poseStream = await navigator.mediaDevices.getUserMedia({
                    video: { width: { ideal: 640 }, height: { ideal: 480 }, facingMode: 'user' }
                });
                poseVideo.srcObject = poseStream;
                poseVideo.onloadedmetadata = () => {
                    poseCanvas.width = poseVideo.videoWidth;
                    poseCanvas.height = poseVideo.videoHeight;
                    poseStatus.textContent = 'Detecting pose...';
                    runPoseDetection();
                };
            } catch (err) {
                console.error('Error starting pose webcam:', err);
                poseStatus.textContent = 'Error accessing webcam.';
            }
        }

        const poseConnections = [
            ['left_shoulder', 'right_shoulder'],
            ['left_shoulder', 'left_elbow'],
            ['left_elbow', 'left_wrist'],
            ['right_shoulder', 'right_elbow'],
            ['right_elbow', 'right_wrist'],
            ['left_shoulder', 'left_hip'],
            ['right_shoulder', 'right_hip'],
            ['left_hip', 'right_hip'],
            ['left_hip', 'left_knee'],
            ['left_knee', 'left_ankle'],
            ['right_hip', 'right_knee'],
            ['right_knee', 'right_ankle'],
        ];

        function drawPose(keypoints) {
            poseCtx.clearRect(0, 0, poseCanvas.width, poseCanvas.height);
            poseCtx.lineWidth = 3;
            poseCtx.strokeStyle = '#22d3ee';
            poseCtx.fillStyle = '#22d3ee';

            const kpByName = {};
            keypoints.forEach(kp => {
                kpByName[kp.name] = kp;
                if (kp.score > 0.4) {
                    poseCtx.beginPath();
                    poseCtx.arc(kp.x, kp.y, 4, 0, 2 * Math.PI);
                    poseCtx.fill();
                }
            });

            poseConnections.forEach(([a, b]) => {
                const kp1 = kpByName[a];
                const kp2 = kpByName[b];
                if (kp1 && kp2 && kp1.score > 0.4 && kp2.score > 0.4) {
                    poseCtx.beginPath();
                    poseCtx.moveTo(kp1.x, kp1.y);
                    poseCtx.lineTo(kp2.x, kp2.y);
                    poseCtx.stroke();
                }
            });
        }

        async function runPoseDetection() {
            if (!poseModel) {
                await loadPoseModel();
                if (!poseModel) return;
            }
            const estimate = async () => {
                if (!poseVideo || poseVideo.readyState < 2) {
                    poseAnimId = requestAnimationFrame(estimate);
                    return;
                }
                try {
                    const poses = await poseModel.estimatePoses(poseVideo);
                    if (poses && poses[0] && poses[0].keypoints) {
                        drawPose(poses[0].keypoints);
                    } else {
                        poseCtx.clearRect(0, 0, poseCanvas.width, poseCanvas.height);
                    }
                } catch (err) {
                    console.error('Pose detection error:', err);
                }
                poseAnimId = requestAnimationFrame(estimate);
            };
            estimate();
        }

        function stopPoseDemo() {
            if (poseAnimId) cancelAnimationFrame(poseAnimId);
            poseAnimId = null;
            poseCtx.clearRect(0, 0, poseCanvas.width, poseCanvas.height);
            if (poseStream) {
                poseStream.getTracks().forEach(t => t.stop());
                poseStream = null;
            }
            poseVideo.srcObject = null;
            poseStatus.textContent = 'Stopped.';
        }

        if (poseButton) {
            poseButton.addEventListener('click', async () => {
                if (poseAnimId || poseStream) {
                    stopPoseDemo();
                } else {
                    await loadPoseModel();
                    await startPoseWebcam();
                }
            });
        }

        // --- Person Segmentation Demo (BodyPix) ---
        const segVideo = document.getElementById('seg-video');
        const segCanvas = document.getElementById('seg-canvas');
        const segButton = document.getElementById('seg-button');
        const segStatus = document.getElementById('seg-status');
        const segCtx = segCanvas.getContext('2d');

        let bodyPixModel = null;
        let segAnimId = null;
        let segStream = null;

        async function loadBodyPixModel() {
            if (bodyPixModel) return bodyPixModel;
            segStatus.textContent = 'Loading segmentation model...';
            try {
                bodyPixModel = await bodyPix.load({
                    architecture: 'MobileNetV1',
                    outputStride: 16,
                    multiplier: 0.75,
                    quantBytes: 2
                });
                segStatus.textContent = 'Model ready.';
            } catch (err) {
                console.error('Error loading BodyPix model:', err);
                segStatus.textContent = 'Error loading model.';
            }
            return bodyPixModel;
        }

        async function startSegWebcam() {
            if (!navigator.mediaDevices?.getUserMedia) {
                segStatus.textContent = 'Webcam not supported.';
                return;
            }
            try {
                segStream = await navigator.mediaDevices.getUserMedia({
                    video: { width: { ideal: 640 }, height: { ideal: 480 }, facingMode: 'user' }
                });
                segVideo.srcObject = segStream;
                segVideo.onloadedmetadata = () => {
                    segCanvas.width = segVideo.videoWidth;
                    segCanvas.height = segVideo.videoHeight;
                    segStatus.textContent = 'Segmenting person...';
                    runSegmentation();
                };
            } catch (err) {
                console.error('Error starting segmentation webcam:', err);
                segStatus.textContent = 'Error accessing webcam.';
            }
        }

        async function runSegmentation() {
            if (!bodyPixModel) {
                await loadBodyPixModel();
                if (!bodyPixModel) return;
            }

            const step = async () => {
                if (!segVideo || segVideo.readyState < 2) {
                    segAnimId = requestAnimationFrame(step);
                    return;
                }
                try {
                    const segmentation = await bodyPixModel.segmentPerson(segVideo, {
                        internalResolution: 'medium',
                        segmentationThreshold: 0.7
                    });

                    // Draw video frame first
                    segCtx.drawImage(segVideo, 0, 0, segCanvas.width, segCanvas.height);

                    // Create mask
                    const mask = bodyPix.toMask(
                        segmentation,
                        { r: 34, g: 211, b: 238, a: 200 },  // person: cyan
                        { r: 15, g: 23, b: 42, a: 120 }      // background: dark
                    );

                    segCtx.putImageData(mask, 0, 0);
                } catch (err) {
                    console.error('Segmentation error:', err);
                }
                segAnimId = requestAnimationFrame(step);
            };
            step();
        }

        function stopSegDemo() {
            if (segAnimId) cancelAnimationFrame(segAnimId);
            segAnimId = null;
            segCtx.clearRect(0, 0, segCanvas.width, segCanvas.height);
            if (segStream) {
                segStream.getTracks().forEach(t => t.stop());
                segStream = null;
            }
            segVideo.srcObject = null;
            segStatus.textContent = 'Stopped.';
        }

        if (segButton) {
            segButton.addEventListener('click', async () => {
                if (segAnimId || segStream) {
                    stopSegDemo();
                } else {
                    await loadBodyPixModel();
                    await startSegWebcam();
                }
            });
        }

        // --- Resume & JD Analyzer (Gemini) ---
        const analyzeBtn = document.getElementById('analyze-resume-button');
        const jdInput = document.getElementById('jd-input');
        const resumeInput = document.getElementById('resume-input');
        const resumeOutput = document.getElementById('resume-analysis-output');

        if (analyzeBtn && jdInput && resumeInput && resumeOutput) {
            analyzeBtn.addEventListener('click', async () => {
                const jdText = jdInput.value.trim();
                const resumeText = resumeInput.value.trim();

                if (!jdText || !resumeText) {
                    resumeOutput.textContent =
                        'Please paste both the Job Description and your Resume text first.';
                    return;
                }

                if (!GEMINI_API_KEY || GEMINI_API_KEY === "YOUR_GEMINI_API_KEY_HERE") {
                    resumeOutput.textContent =
                        'Please configure a valid Gemini API key in the source code to enable this demo.';
                    return;
                }

                resumeOutput.textContent = 'Analyzing with Gemini...';

                const analysisPrompt = `
You are an AI career assistant embedded in an ML engineer's personal portfolio site.

TASK:
1. Estimate match between the candidate's resume and the job description (0–100%).
2. List 3–5 key strengths where the candidate matches the JD.
3. List 3–5 gaps or missing skills.
4. Suggest 3 concrete resume bullet points the candidate can add to better fit this JD.

Return in a clean, readable format with headings.

JOB DESCRIPTION:
${jdText}

RESUME:
${resumeText}
                `.trim();

                try {
                    const response = await fetchWithBackoff(GEMINI_TEXT_API_URL, {
                        method: "POST",
                        headers: { "Content-Type": "application/json" },
                        body: JSON.stringify({
                            contents: [
                                {
                                    role: "user",
                                    parts: [{ text: analysisPrompt }]
                                }
                            ]
                        })
                    });

                    const data = await response.json();

                    if (!response.ok) {
                        console.error('Resume analyzer error:', data);
                        let msg = data.error?.message || `Request failed with status ${response.status}`;
                        if (msg.includes("API_KEY_HTTP_REFERRER_BLOCKED")) {
                            msg = "Domain is not authorized for this API key. Please adjust key restrictions in Google Cloud Console.";
                        }
                        resumeOutput.textContent = msg;
                        return;
                    }

                    const text = data?.candidates?.[0]?.content?.parts?.[0]?.text?.trim();
                    if (!text) {
                        resumeOutput.textContent = 'Empty response from model.';
                        return;
                    }

                    resumeOutput.textContent = text;
                } catch (err) {
                    console.error('Error in resume analyzer:', err);
                    resumeOutput.textContent =
                        'Error: Could not reach Gemini. Please try again later.';
                }
            });
        }
    });
</script>
</body>
</html>
